# 论文读后感

<center>19301100 童耀乐<center>

### 一、MapReduce

在选修非关系型数据库这门课之前，我对google的MapReduce是一无所知，从来没有听说过。在老师的要求下，我阅读了包含MapReduce在内的google三大核心技术，确实受益匪浅。虽然可能还有些地方看的不是很明白，但基本的架构以及论文想要传达的一些思想我可以说是有所了解。

先来讲讲MapReduce。MapReduce 是一种编程模型和相关的实现，用于处理和生成大型数据集。概念"Map"和"Reduce"，是它们的主要思想，都是从函数式编程语言里借来的，还有从矢量编程语言里借来的特性。它极大地方便了编程人员在不会分布式并行编程的情况下，将自己的程序运行在[分布式系统](https://baike.baidu.com/item/分布式系统/4905336)上。 当前的软件实现是指定一个Map函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce函数，用来保证所有映射的键值对中的每一个共享相同的键组。

以上大概就是从论文里可以挖掘出来的对于MapReduce的一些较为官方的解释。在我读完论文之后，用通俗的话大概总结一下MapReduce：它是将一个大作业拆分为多个小作业的框架（规模不同），然后用户需要做的就是决定拆成多少份，以及定义作业本身。这是一个伟大的发明。因为在我们的日常生活中，拥有海量的数据，是一台电脑无法处理完的。那么用多台电脑分别去把数据拆分进行处理，正是MapReduce这个技术所提出来的分布式的思想。这在计算机领域上是一项伟大的技术发明。

在实现上，MapReduce模型可以有多种不同的实现方式:有的适用于小型的共享内存方式的机器，有的则适用于大型NUMA架构的多处理器的主机，而有的实现方式更适合人型的网络连接集群。在文中，作者主要描述了一个适用 Gougle内部广泛使用的运算环境的实现:用以太网交换机连接、由普通PC机组成的大型网络连接集群。

文中作者强调环境包括：

1.  x86 架构、运行Linux操作系统、双处理器、2-4GB内存的机器。
2. 普通的网络硬件设备，每个机器的带宛为百兆或者千兆，但是远小于网络的平均带宽的一半。
3. 集群中包含成百 上千的机器，因此，机器故障是常态。
4. 存储为廉价的内置 IDE硬盘。
5. 用户提交工作(job)给调度系统。

原文中作者有一个很清晰的图来说明各个调度之间的关系：

![image-20211231162220124](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20211231162220124.png)



Master持有一些数据结构,它储存每一个Map和Reduce任务的状态，以及Worker机器的标识。Maser就像一个数据管道，中向文件存储区域的位畳信息通过这个管道从Map传递到Reduce.因此,对于每个已完成的Map任务, master存儲了Map任务产生的文件存儲区域的大小和位置。当Map任务完成时，Maser 接收到位置和大小的更新信息，这些信息被逐步递增的推送给那些正在工作的Reduce任务。MapReduce中还有着良好的容错机制。因为MapReduce库的设计初衷是使用由成百上千的机器组成的集群来处理超大规模的数据，所以，这个库必须要很好的能够处理机器故障。

MapReduce编程模型在Google内部成功应用于多个领域。原因有这几个方面:首先，由于MapReduce封装了并行处理、容错处理、数据本地化优化、负载均衡等等技术难点的细节，这使得MapReduce库易于使用。即便对于完全没有并行或者分布式系统开发经验的程序员而言，其次，大量不同类型的问题都可以通过MapReduce简单的解决。比如，MapReduce用于生成Google的网络搜索服务所需要的数据、用来排序、用来数据挖掘、用于机器学习，以及很多其它的系统;第三，我们实现了一个在数千台计算机组成的大型集群，上灵活部署运行的MapReduce。这个实现使得有效利用这些丰富的计算资源变得非常简单，因此也适合用来解决Google遇到的其他很多需要大量计算的问题。

总而言之，对于MapReduce这项技术的发明，其意义还是很伟大的。通过这几天阅读文献，我对于MapReduce这个技术有了一定的了解。库的概念，很多，适用于工业化工程化操作，也适合合作，尤适合长拉锯战中的操作，之前关注的大疆，大疆公司会在每年举办robomaster比赛，参赛方都是以团队的形式，他们在完成任务的时候，都是是分工去完成各自的任务。就拿编程来说，可能是分人负责一块，这样开发起来速度就会快很多。如果软件代码只由一位同学写完，那在整个开发过程中操作和开发的速度就会慢很多。因此有着库和函数的概念，会简化高效很多。MapReduce中的思想与运用的技术手段都是很好的。对于库封装的应用；作为一个结构，适用于不同问题，在它们数据大量的前提下，可以达到简单实现；可运行在大型集群，灵活且高效。等等这些，意味着它真的有很大的适用性。在以后的生活中，我也会继续不断跟进了解一些新的技术，继续了解MapReduce这样经典的技术并尝试加以运用，努力掌握在大数据时代背景下的一种经典重要的技术。

### 二、Google File System

一个系统如何设计，一定要依据这个系统想解决什么样的问题，所以确定并分析问题本身，比如何解决问题更关键。如今来看，Google GFS文件系统，是一个面向大规模数据密集型应用的、可伸缩的分布式文件系统。GFS虽然运行在廉价的普遍硬件设备上，但是它依然了提供灾难冗余的能力，为大量客户机提供了高性能的服务。

虽然GFS的设计目标与许多传统的分布式文件系统有很多相同之处，但是在GFS的设计中我们还是以我们对自己的应用的负载情况和技术环境的分析为基础进行的，不管现在还是将来，GFS和早期的分布式文件系统的设想都有明显的不同。所以我们重新审视了传统文件系统在设计上的折中选择，衍生出了完全不同的设计思路。

GFS完全满足了我们对存储的需求。GFS作为存储平台已经被广泛的部署在Google内部，存储我们的服务产生和处理的数据，同时还用于那些需要大规模数据集的研究和开发工作。目前为止，最大的一个集群利用数千台机器的数千个硬盘，提供了数百TB的存储空间，同时为数百个客户机服务。

相对于传统的分布式文件系统，GFS针对Google应用的特点从多个方面进行了简化，从而在一定规模下达到成本、可靠性和性能的最佳平衡。具体来说，它具有以下几个特点。

#### 1．采用中心服务器模式

GFS采用中心服务器模式来管理整个文件系统，可以大大简化设计，从而降低实现难度。Master管理了分布式文件系统中的所有元数据。文件划分为Chunk进行存储，对于Master来说，每个Chunk Server只是一个存储空间。Client发起的所有操作都需要先通过Master才能执行。这样做有许多好处，增加新的Chunk Server是一件十分容易的事情，Chunk Server只需要注册到Master上即可，Chunk Server之间无任何关系。如果采用完全对等的、无中心的模式，那么如何将Chunk Server的更新信息通知到每一个Chunk Server，会是设计的一个难点，而这也将在一定程度上影响系统的扩展性。Master维护了一个统一的命名空间，同时掌握整个系统内Chunk Server的情况，据此可以实现整个系统范围内数据存储的负载均衡。由于只有一个中心服务器，元数据的一致性问题自然解决。当然，中心服务器模式也带来一些固有的缺点，比如极易成为整个系统的瓶颈等。GFS采用多种机制来避免Master成为系统性能和可靠性上的瓶颈，如尽量控制元数据的规模、对Master进行远程备份、控制信息和数据分流等。

#### 2．不缓存数据

缓存（Cache）机制是提升文件系统性能的一个重要手段，通用文件系统为了提高性能，一般需要实现复杂的缓存机制。GFS文件系统根据应用的特点，没有实现缓存，这是从必要性和可行性两方面考虑的。从必要性上讲，客户端大部分是流式顺序读写，并不存在大量的重复读写，缓存这部分数据对系统整体性能的提高作用不大；而对于Chunk Server，由于GFS的数据在Chunk Server上以文件的形式存储，如果对某块数据读取频繁，本地的文件系统自然会将其缓存。从可行性上讲，如何维护缓存与实际数据之间的一致性是一个极其复杂的问题，在GFS中各个Chunk Server的稳定性都无法确保，加之网络等多种不确定因素，一致性问题尤为复杂。此外由于读取的数据量巨大，以当前的内存容量无法完全缓存。对于存储在Master中的元数据，GFS采取了缓存策略，GFS中Client发起的所有操作都需要先经过Master。Master需要对其元数据进行频繁操作，为了提高操作的效率，Master的元数据都是直接保存在内存中进行操作。同时采用相应的压缩机制降低元数据占用空间的大小，提高内存的利用率。

#### 3．在用户态下实现

文件系统作为操作系统的重要组成部分，其实现通常位于操作系统底层。以Linux为例，无论是本地文件系统如Ext3文件系统，还是分布式文件系统如Lustre等，都是在内核态实现的。在内核态实现文件系统，可以更好地和操作系统本身结合，向上提供兼容的POSIX接口。然而，GFS却选择在用户态下实现，主要基于以下考虑。

在用户态下实现，直接利用操作系统提供的POSIX编程接口就可以存取数据，无需了解操作系统的内部实现机制和接口，从而降低了实现的难度，并提高了通用性。

#### 4．只提供专用接口

通常的分布式文件系统一般都会提供一组与POSIX规范兼容的接口。其优点是应用程序可以通过操作系统的统一接口来透明地访问文件系统，而不需要重新编译程序。GFS在设计之初，是完全面向Google的应用的，采用了专用的文件系统访问接口。接口以库文件的形式提供，应用程序与库文件一起编译，Google应用程序在代码中通过调用这些库文件的API，完成对GFS文件系统的访问。



总的来说，Google文件系统展示了一个在一般硬件上支持大规模数据处理工作的核心特性。一些设计决定都是根据我们的特殊环境定制的，但许多还是能够用于类似规模和成本的数据处理任务。

首先，根据我们当前的和预期的应用工作量和技术环境来重新考察传统的文件系统。观察结果引导出一个完全不同的设计思路。我们将组件失败看成是普通现象而非异常情况，对于向大文件进行追加（可能是并行的）和读操作（通常是序列化的）来进行优化，以及通过扩展接口和放松限制来改进整个系统。

我们的系统通过持续健康、复制关键数据以及快速自动的恢复来提供容错。块复制允许我们容忍块服务器的错误。这些错误的频率促进了一个新奇的在线修复机制，它定期的、透明的修复损坏的数据，并尽快对丢失的副本进行补偿。此外，在磁盘或IDE子系统层上，我们使用了校验和来探测数据是否损坏，在这种规模的磁盘数量上，这种问题十分普遍。

对于进行许多并行读写操作的各种任务，我们的设计提供了很高的总吞吐量。我们通过将控制流和数据流分开来实现这个目标，控制流直接经过Master，数据流块服务器和客户端之间传输。通过大的块大小和在数据修改操作中授予主拷贝一个块租约，能够最小化Master涉及的操作，这使简单的、中心的Master不会太可能成为瓶颈。我们相信我们的网络协议栈的改进可以提高从客户端看到的写入吞吐量的限制。

GFS很好的满足了我们的存储需求，并作为存储平台在Google中得到广泛使用，无论在研究和开发，还是生产数据处理上。它是我们持续创新和攻克整个web范围的难题的一个重要工具。

### 三、BigTable

在选修非关系型数据库这门课之前，我对google的BigTable是一无所知，从来没有听说过。在老师的要求下，我阅读了包含BigTable在内的google三大核心技术，确实受益匪浅。虽然可能还有些地方看的不是很明白，但基本的架构以及论文想要传达的一些思想我可以说是有所了解。

BigTable是谷歌设计的数据存储系统，它是全球化的、分布式的、持久化存储的、多维度排序的(数个层级)、可以被部署在几千台计算机上用来处理海量数据的一种非关系型的数据库。他拥有适用性广泛（全谷歌平台）、可扩展（大量数据）、高效处理性能和高可靠性等特点。Bigtable广泛使用于谷歌的产品，适应谷歌不同平台不同的要求，可以适应大量数据快速处理，可以把数据快速输送给大量用户，可以在一台或者几台大型服务器使用，也可以在几千台小计算机配置。Bigtable在很多方面和数据库很类似，很多数据库功能它都可以实现，但是它以不同于数据库的方式在连接。
		Bigtable为客户提供了简单的数据模型，利用这个模型，用户可以自己随意控制数据的尺寸读取，它是动态设置而不是静态。用户可以自己决定文件储存在什么地方，以什么样的方式储存，数据会标明储存位置，名字则可以由用户任意决定。Bigtable将存储的数据都视为字符串来进行处理，但是Bigtable本身不去解读或者修改这些东西，而交由客户程序处理。客户程序会把储存的数据用一定方式与这些字符串相关联以供使用，而这些都可以由用户自己去控制如何操作设置。最后， BigTable也可以设置数据在哪里储存，比如储存在你的硬盘还是内存之中。他拥有以下六个特点:
1、全球大规模海量数据；
2、各地的几千台计算机同时进行都可以，效率极高；
3、没有太大限制，扩展空间高；
4、微机都可以使用；
5、适合存取，不适合编辑；
6、不适用于传统关系型数据库；

后面paper就开始介绍这个系统的一些细节组成部分。有数据模型的细节介绍，写了API客户端，写了bigtable的主要构件——使用GFS来存储日志文件和数据文件，内部存储数据的文件是Google SSTable格式的，它还依赖一个高可用的、序列化的分布式锁服务组件叫Chubby.

Bigtable的功能实现依靠三个主要部分：一个可以连接到每个用户的库文件，一个主要服务器控制其他，许多目录服务器来分配文件的储存。主服务器负责把目录分配到目录服务器上，检测目录服务器的改变，平衡目录服务器负载，对谷歌文件系统进行垃圾收集，还有控制不同列族等内容的改变。每一个目录服务器都管理一组目录（一个目录服务器负责很多目录的管理）。目录服务器处理对该服务器上目录的读写请求，也就是不同目录对应的不同文件，也会将过大超过储存极限的目录分裂为多个目录来储存数据。一个Bigtable集群存储了大量的表。每一个表都由一组目录构成，每一个目录包含一定的储存数据。初始情况下，每个表仅包含一个目录。随着表内的数据的增加，它会自动分裂成多个目录来储存，默认每个表可以达到1GB。目录以树状形式储存，读取也是有主干具体到某个文件所在的叶，一层层读取下去，每一层目录都有个记录位置关键词下面，主服务器会缓存这个关键词以供查询和备份。每个目录只能分配给一个目录服务器，主服务器会自动检测目录服务器并实现合理分配利用。

通过阅读这篇论文，我发自肺腑的想为世界科技的日益强大点个赞，我也十分敬佩那些为世界科技在默默无闻做出贡献的研发人员，他们把自己的青春都给了科研，也让我们体验到了科技带给我们的便利。阅读这篇论文，真的为我打开了一扇新的大门，虽然还有相当一部分地方没有读懂，但我也要努力理解并在以后将其运用于实践之中，尽自己的绵薄之力运用已有技术，大胆创新，为社会国家建设奉献自己的一份力量！
